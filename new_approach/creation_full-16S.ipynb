{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f09cdde",
   "metadata": {},
   "source": [
    "# Create full 16S database by merging SILVA, RDP and Greengenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fd9dab8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-06T18:27:05.660804Z",
     "start_time": "2023-04-06T18:27:05.355111Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils import check_dir\n",
    "import utils\n",
    "from os.path import join, exists, basename, splitext\n",
    "import pickle\n",
    "from re import sub, search\n",
    "from copy import copy\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8930a6",
   "metadata": {},
   "source": [
    "## Original Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a70da45",
   "metadata": {},
   "source": [
    "### Download original files\n",
    "We download greengenes and silva 16S database (99NR).\n",
    "\n",
    "- Greengenes v13_8 (99)\n",
    "- Silva v138 SSURef NR99 515F/806R (qiime2  pre-formatted)\n",
    "\n",
    "RDP database was obtained from the `RDPClassifier_16S_trainsetNo18_QiimeFormat.zip` file in this [website](https://sourceforge.net/projects/rdp-classifier/files/RDP_Classifier_TrainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc6e1c7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-06T18:24:50.083740Z",
     "start_time": "2023-04-06T18:24:50.075048Z"
    }
   },
   "outputs": [],
   "source": [
    "# Environment\n",
    "or_dir = \"original_db\"\n",
    "check_dir(or_dir)\n",
    "\n",
    "download_db = {\"greengenes\":[\n",
    "                (\"http://greengenes.microbio.me/greengenes_release/gg_13_8_otus/rep_set/99_otus.fasta\",\n",
    "                 join(or_dir, \"gg_13_8_NR99_seqs.fasta\")),\n",
    "                (\"http://greengenes.microbio.me/greengenes_release/gg_13_8_otus/taxonomy/99_otu_taxonomy.txt\",\n",
    "                join(or_dir, \"gg_13_8_NR99_taxa.txt\"))\n",
    "],\n",
    "             \"silva\":[\n",
    "                (\"https://data.qiime2.org/2022.8/common/silva-138-99-seqs.qza\",\n",
    "                join(or_dir, \"silva_138_NR99_seqs.qza\")),\n",
    "                (\"https://data.qiime2.org/2022.8/common/silva-138-99-tax.qza\",\n",
    "                join(or_dir, \"silva_138_NR99_taxa.qza\"))\n",
    "            ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1073472d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-06T18:24:53.053110Z",
     "start_time": "2023-04-06T18:24:52.468275Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-04-06 20:24:52--  http://%7Blink%7D/\n",
      "Resolving {link} ({link})... failed: Name or service not known.\n",
      "wget: unable to resolve host address ‘{link}’\n",
      "File ‘{outname}’ already there; not retrieving.\n",
      "File ‘{outname}’ already there; not retrieving.\n",
      "File ‘{outname}’ already there; not retrieving.\n"
     ]
    }
   ],
   "source": [
    "for db, values in download_db.items():\n",
    "    for item in values:\n",
    "        link = item[0]\n",
    "        path = item[1]\n",
    "        \n",
    "        ! wget -nc {link} -O {outname}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee23cb1",
   "metadata": {},
   "source": [
    "### Decompress silva qza files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e92fe2d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-06T18:25:10.594Z"
    }
   },
   "outputs": [],
   "source": [
    "# Outnames\n",
    "silva_fasta_qza = download_db['silva'][0][1]\n",
    "silva_taxa_qza = download_db['silva'][1][1]\n",
    "silva_fasta = f\"{splitext(download_db['silva'][0][1])[0]}.fasta\"\n",
    "silva_txt = f\"{splitext(download_db['silva'][1][1])[0]}.txt\"\n",
    "\n",
    "utils.unqza(silva_fasta_qza, silva_fasta)\n",
    "utils.unqza(silva_taxa_qza, silva_taxa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c025b6",
   "metadata": {},
   "source": [
    "### Original databases files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f504e81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-06T18:27:09.538113Z",
     "start_time": "2023-04-06T18:27:09.520031Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'silva': ['original_db/silva_138_NR99_seqs.fasta',\n",
       "  'original_db/silva_138_NR99_taxa.txt'],\n",
       " 'gg': ['original_db/gg_13_8_NR99_seqs.fasta',\n",
       "  'original_db/gg_13_8_NR99_taxa.txt'],\n",
       " 'gp_97': ['original_db/gp_97_otus_OLD.fasta',\n",
       "  'original_db/gp_97_taxa_OLD.txt'],\n",
       " 'rdp': ['original_db/RDPClassifier_16S_trainsetNo18_QiimeFormat/RefOTUs.fa',\n",
       "  'original_db/RDPClassifier_16S_trainsetNo18_QiimeFormat/Ref_taxonomy.txt'],\n",
       " 'itgdb': ['original_db/taxa_itgdb_seq.fasta',\n",
       "  'original_db/taxa_itgdb_taxa.txt']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "or_dir = \"original_db\"\n",
    "databases = {\n",
    "    'silva': [join(or_dir, 'silva_138_NR99_seqs.fasta'),\n",
    "             join(or_dir, 'silva_138_NR99_taxa.txt')\n",
    "             ],\n",
    "    'gg': [join(or_dir, \"gg_13_8_NR99_seqs.fasta\"),\n",
    "           join(or_dir, \"gg_13_8_NR99_taxa.txt\")\n",
    "          ],\n",
    "    'gp_97': [join(or_dir, \"gp_97_otus_OLD.fasta\"),\n",
    "           join(or_dir, \"gp_97_taxa_OLD.txt\")       \n",
    "             ],\n",
    "    'rdp': [join(or_dir, 'RDPClassifier_16S_trainsetNo18_QiimeFormat', 'RefOTUs.fa' ),\n",
    "            join(or_dir, 'RDPClassifier_16S_trainsetNo18_QiimeFormat', 'Ref_taxonomy.txt' )\n",
    "           ],\n",
    "    \"itgdb\": [join(or_dir, \"taxa_itgdb_seq.fasta\"),\n",
    "              join(or_dir, \"taxa_itgdb_taxa.txt\")\n",
    "             ]\n",
    "}\n",
    "\n",
    "databases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37398eff",
   "metadata": {},
   "source": [
    "### Inspect original files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63119035",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-06T18:27:23.387024Z",
     "start_time": "2023-04-06T18:27:12.572950Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "436681 original_db/silva_138_NR99_taxa.txt\n",
      "203452 original_db/gg_13_8_NR99_taxa.txt\n",
      "127414 original_db/gp_97_taxa_OLD.txt\n",
      "21195 original_db/RDPClassifier_16S_trainsetNo18_QiimeFormat/Ref_taxonomy.txt\n",
      "110780 original_db/taxa_itgdb_taxa.txt\n"
     ]
    }
   ],
   "source": [
    "for key, items in databases.items():\n",
    "    taxafile = items[1]\n",
    "    ! wc -l {taxafile}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553bc9c4",
   "metadata": {},
   "source": [
    "### Unify format of original dbs\n",
    "Here we will save a formatted taxonomy file for Greengenes, RDP, the old gp97 database and the ITGDB-16S database from this [paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9580931/?report=classic). Those databases genus and species format is `g__Genus; s__specie`, while the format of the database we are creating and the Silva database is `g__Genus; s__Genus_specie`, so in this step we are only changing this format to make all the databases comparable. \n",
    "\n",
    "In greengenes and ITGDB database we will also include the taxonomy header required by qiime2: `Feature ID\\tTaxon`.\n",
    "\n",
    "In RDP, we are also converting the sequence to uppercase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c895b36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-06T18:27:29.086493Z",
     "start_time": "2023-04-06T18:27:29.076114Z"
    }
   },
   "outputs": [],
   "source": [
    "dbs_format = {\n",
    "    'silva': [join(or_dir, 'silva_138_NR99_seqs.fasta'),\n",
    "             join(or_dir, 'silva_138_NR99_taxa.txt')\n",
    "             ],\n",
    "    'gg': [join(or_dir, \"gg_13_8_NR99_seqs.fasta\"),\n",
    "           join(or_dir, \"gg_13_8_NR99_taxa_spformat.txt\")\n",
    "          ],\n",
    "    'gp_97': [join(or_dir, \"gp_97_otus_OLD.fasta\"),\n",
    "           join(or_dir, \"gp_97_taxa_OLD_spformat.txt\")       \n",
    "             ],\n",
    "    'rdp': [ join(or_dir, 'rdp_16S_seqs.fasta' ),\n",
    "            join(or_dir, 'rdp_16S_taxa_spformat.txt' )\n",
    "           ],\n",
    "    \"itgdb\": [join(or_dir, \"taxa_itgdb_seq.fasta\"),\n",
    "              join(or_dir, \"taxa_itgdb_taxa_spformat.txt\")\n",
    "             ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9e7b264",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-06T18:29:46.612823Z",
     "start_time": "2023-04-06T18:27:32.779741Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formating gg\n",
      "Formating gp_97\n",
      "28362 entries without sequence or taxa information were deleted\n",
      "Formating rdp\n",
      "Formating itgdb\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "for db, files in databases.items():\n",
    "    if db == 'silva':\n",
    "        continue\n",
    "    \n",
    "    # load database\n",
    "    print(f'Formating {db}')\n",
    "    database = utils.load_db_from_files(files[1], files[0])\n",
    "    \n",
    "    # format species level\n",
    "    format_sp = ['gg', 'rdp', 'gp_97', 'itgdb']\n",
    "    \n",
    "    if db in format_sp:\n",
    "        for ID, items in database.items():\n",
    "            if items['taxa'][6] != \"\":\n",
    "                items['taxa'][6] = '_'.join([items['taxa'][5], items['taxa'][6]])\n",
    "\n",
    "    # convert sequence to uppercase\n",
    "    if db == \"rdp\":\n",
    "        for ID, items in database.items():\n",
    "            items['seq'] = items['seq'].upper()\n",
    "            \n",
    "            \n",
    "    # download formatted files\n",
    "    utils.download_db_from_dict(database, dbs_format[db][1], dbs_format[db][0])\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d779dd",
   "metadata": {},
   "source": [
    "#### Create qza files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05af8bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbs_format_qza = {}\n",
    "\n",
    "for db, files in dbs_format.items():\n",
    "    dbs_format_qza[db] = [\"\", \"\"]\n",
    "    for i, file in enumerate(files):\n",
    "        qza = utils.file_to_qza(file)\n",
    "        dbs_format_qza[db][i] = qza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ad0513b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'silva': ['original_db/silva_138_NR99_seqs.qza',\n",
       "  'original_db/silva_138_NR99_taxa.qza'],\n",
       " 'gg': ['original_db/gg_13_8_NR99_seqs.qza',\n",
       "  'original_db/gg_13_8_NR99_taxa_spformat.qza'],\n",
       " 'gp_97': ['original_db/gp_97_otus_OLD.qza',\n",
       "  'original_db/gp_97_taxa_OLD_spformat.qza'],\n",
       " 'rdp': ['original_db/rdp_16S_seqs.qza',\n",
       "  'original_db/rdp_16S_taxa_spformat.qza'],\n",
       " 'itgdb': ['original_db/taxa_itgdb_seq.qza',\n",
       "  'original_db/taxa_itgdb_taxa_spformat.qza']}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbs_format_qza"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2778f24",
   "metadata": {},
   "source": [
    "## Create merged database\n",
    "\n",
    "We will create our database by merging RDP, Silva and Greengenes databases, in this order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a2a43f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'silva': ['original_db/silva_138_NR99_seqs.fasta',\n",
       "  'original_db/silva_138_NR99_taxa.txt'],\n",
       " 'gg': ['original_db/gg_13_8_NR99_seqs.fasta',\n",
       "  'original_db/gg_13_8_NR99_taxa_spformat.txt'],\n",
       " 'rdp': ['original_db/rdp_16S_seqs.fasta',\n",
       "  'original_db/rdp_16S_taxa_spformat.txt']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "base_dbs = deepcopy(dbs_format)\n",
    "base_dbs = {db: items for db, items in base_dbs.items() if db in ['rdp','silva','gg']}\n",
    "base_dbs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b10ae22",
   "metadata": {},
   "source": [
    "### Filter base databases by taxonomy \n",
    "First we will filter the base databases to remove entries with taxonomies that we dont want to include in our database. \n",
    "\n",
    "#### Only Bacteria and Archae\n",
    "Silva database contains Eukaryota and viruses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51236b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "389016 original_db/silva_138_NR99_taxa_BactArch.txt\n",
      "203452 original_db/gg_13_8_NR99_taxa_spformat_BactArch.txt\n",
      "21194 original_db/rdp_16S_taxa_spformat_BactArch.txt\n"
     ]
    }
   ],
   "source": [
    "# Only modify taxa files\n",
    "\n",
    "for db, files in base_dbs.items():\n",
    "    taxa_file = files[1]\n",
    "    taxa_out = f\"{splitext(taxa_file)[0]}_BactArch.txt\"\n",
    "    ! grep \"Bacteria\\|Archaea\" {taxa_file} > {taxa_out}\n",
    "    \n",
    "    # print\n",
    "    ! wc -l {taxa_out}\n",
    "    \n",
    "    # rename base files\n",
    "    base_dbs[db][1] = taxa_out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c4b376",
   "metadata": {},
   "source": [
    "#### Filter weird names\n",
    "Eliminate weird names (e.g. metagenome, uncultured,...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "396b11b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78415 original_db/silva_138_NR99_taxa_BactArch_filtweird.txt\n",
      "201585 original_db/gg_13_8_NR99_taxa_spformat_BactArch_filtweird.txt\n",
      "20944 original_db/rdp_16S_taxa_spformat_BactArch_filtweird.txt\n"
     ]
    }
   ],
   "source": [
    "patterns_file = \"discard_patterns.txt\"\n",
    "\n",
    "# Only modify taxa files\n",
    "\n",
    "for db, files in base_dbs.items():\n",
    "    taxa_file = files[1]\n",
    "    taxa_out = f\"{splitext(taxa_file)[0]}_filtweird.txt\"\n",
    "\n",
    "    ! grep -vf {patterns_file} {taxa_file} > {taxa_out}\n",
    "    \n",
    "    # print\n",
    "    ! wc -l {taxa_out}\n",
    "    \n",
    "    # rename base files\n",
    "    base_dbs[db][1] = taxa_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d8f0e4",
   "metadata": {},
   "source": [
    "#### Filter unknown species\n",
    "Only database greengenes contains unknown species, but in any, we will pass the filter to both files. We suspect that unknown species from SILVA database were already remove with the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2feeb7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78415 original_db/silva_138_NR99_taxa_BactArch_filtweird_filtsp.txt\n",
      "20443 original_db/gg_13_8_NR99_taxa_spformat_BactArch_filtweird_filtsp.txt\n",
      "20151 original_db/rdp_16S_taxa_spformat_BactArch_filtweird_filtsp.txt\n"
     ]
    }
   ],
   "source": [
    "# Only modify taxa files\n",
    "for db, files in base_dbs.items():\n",
    "    taxa_file = files[1]\n",
    "    taxa_out = f\"{splitext(taxa_file)[0]}_filtsp.txt\"\n",
    "\n",
    "    ! grep -v \"s__$\" {taxa_file} > {taxa_out}\n",
    "    \n",
    "    # print\n",
    "    ! wc -l {taxa_out}\n",
    "    \n",
    "    # rename base files\n",
    "    base_dbs[db][1] = taxa_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e0335f",
   "metadata": {},
   "source": [
    "### Homogenize taxa nomenclature\n",
    "#### Read db files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cedb391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "358265 entries without sequence or taxa information were deleted\n",
      "183009 entries without sequence or taxa information were deleted\n",
      "1044 entries without sequence or taxa information were deleted\n"
     ]
    }
   ],
   "source": [
    "base_dbs_dict = {db: utils.load_db_from_files(paths[1], paths[0]) for db, paths in base_dbs.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4532efba",
   "metadata": {},
   "source": [
    "#### Save process  \n",
    "Now we are going to change the conda environment to run some especific python modules, therefore we will save our dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cdc347be",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbs_obj = join(or_dir, \"dbs_dict.pickle\")\n",
    "pickle.dump(base_dbs_dict, open(dbs_obj, 'wb' ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cbd551",
   "metadata": {},
   "source": [
    "#### CHANGE TO ETE3 ENVIRONMENT \n",
    "To switch between conda environments see [here](https://towardsdatascience.com/get-your-conda-environment-to-show-in-jupyter-notebooks-the-easy-way-17010b76e874). We will use the [ete3](http://etetoolkit.org/download/) python package\n",
    "to obtain the lastest version of NCBI taxonomy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "144256e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from os.path import join, basename\n",
    "import update_taxonomy\n",
    "from re import search\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f52e4f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "or_dir = \"original_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40a6157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbs_obj = join(or_dir, \"dbs_dict.pickle\")\n",
    "dbs = pickle.load(open(dbs_obj, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80eee8fc",
   "metadata": {},
   "source": [
    "#### Update taxonomy according NCBI (ete3 module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76af1df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original size of silva: 78415\n",
      "Final size of silva: 76932\n",
      "Total found taxa: 76893\n",
      "64.62980294285863 % of the taxa changed and was updated\n",
      "\n",
      "Original size of gg: 20443\n",
      "Final size of gg: 20441\n",
      "Total found taxa: 20441\n",
      "57.365099554816304 % of the taxa changed and was updated\n",
      "\n",
      "Original size of rdp: 20151\n",
      "Final size of rdp: 20151\n",
      "Total found taxa: 20151\n",
      "49.54096570889782 % of the taxa changed and was updated\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "\n",
    "not_found_taxa = []\n",
    "\n",
    "for key, db in dbs.items():\n",
    "    # Information\n",
    "    found_taxa = 0\n",
    "    change_taxa = 0\n",
    "    len_original_db = len(db)\n",
    "    print(f\"Original size of {key}: {len_original_db}\")\n",
    "    \n",
    "    # Copy database\n",
    "    db_copy = db.copy()\n",
    "    \n",
    "    \n",
    "    for ID, items in db.items():\n",
    "        taxa = items['taxa']\n",
    "        new_taxa = update_taxonomy.update_taxonomy_ncbi(taxa, join_taxa = False)\n",
    "        \n",
    "        \n",
    "        if new_taxa:\n",
    "            # check if taxa was changed\n",
    "            if new_taxa != taxa:\n",
    "                change_taxa +=1\n",
    "                \n",
    "#                 if not any('Candidatus' in tax for tax in new_taxa):\n",
    "#                     print(\"old:\", taxa)\n",
    "#                     print(\"new:\", new_taxa)\n",
    "                    \n",
    "                \n",
    "        \n",
    "                \n",
    "            # check if there is still species level\n",
    "            if new_taxa[6]:\n",
    "                # replace taxa and count it\n",
    "                db[ID]['taxa'] = new_taxa\n",
    "                found_taxa = found_taxa + 1 \n",
    "            else:\n",
    "                db_copy.pop(ID)\n",
    "                               \n",
    "        else:\n",
    "            not_found_taxa.append(taxa)\n",
    "    \n",
    "    dbs[key] = db_copy\n",
    "    \n",
    "\n",
    "    # Information\n",
    "    len_db = len(db_copy)\n",
    "    print(f\"Final size of {key}: {len_db}\")\n",
    "    print(f\"Total found taxa: {found_taxa}\")\n",
    "    print(f\"{(change_taxa/len_db)*100} % of the taxa changed and was updated\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9145da84",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Bacteria',\n",
       "  'Caldatribacteriota',\n",
       "  'JS1',\n",
       "  'JS1',\n",
       "  'JS1',\n",
       "  'JS1',\n",
       "  'benzene_mineralizing'],\n",
       " ['Bacteria',\n",
       "  'LCP-89',\n",
       "  'LCP-89',\n",
       "  'LCP-89',\n",
       "  'LCP-89',\n",
       "  'LCP-89',\n",
       "  'saltmarsh_clone'],\n",
       " ['Bacteria',\n",
       "  'Latescibacterota',\n",
       "  'Latescibacterota',\n",
       "  'Latescibacterota',\n",
       "  'Latescibacterota',\n",
       "  'Latescibacterota',\n",
       "  'saltmarsh_clone'],\n",
       " ['Bacteria',\n",
       "  'Cloacimonadota',\n",
       "  'Cloacimonadia',\n",
       "  'Cloacimonadales',\n",
       "  'PBS-18',\n",
       "  'PBS-18',\n",
       "  'saltmarsh_clone'],\n",
       " ['Bacteria',\n",
       "  'Cloacimonadota',\n",
       "  'Cloacimonadia',\n",
       "  'Cloacimonadales',\n",
       "  'W27',\n",
       "  'W27',\n",
       "  'anaerobic_digester'],\n",
       " ['Bacteria',\n",
       "  'Marinimicrobia_(SAR406_clade)',\n",
       "  'Marinimicrobia_(SAR406_clade)',\n",
       "  'Marinimicrobia_(SAR406_clade)',\n",
       "  'Marinimicrobia_(SAR406_clade)',\n",
       "  'Marinimicrobia_(SAR406_clade)',\n",
       "  'hydrothermal_vent'],\n",
       " ['Bacteria',\n",
       "  'Marinimicrobia_(SAR406_clade)',\n",
       "  'Marinimicrobia_(SAR406_clade)',\n",
       "  'Marinimicrobia_(SAR406_clade)',\n",
       "  'Marinimicrobia_(SAR406_clade)',\n",
       "  'Marinimicrobia_(SAR406_clade)',\n",
       "  'hydrothermal_vent'],\n",
       " ['Bacteria',\n",
       "  'WPS-2',\n",
       "  'WPS-2',\n",
       "  'WPS-2',\n",
       "  'WPS-2',\n",
       "  'WPS-2',\n",
       "  'hydrothermal_vent'],\n",
       " ['Bacteria',\n",
       "  'SAR324_clade(Marine_group_B)',\n",
       "  'SAR324_clade(Marine_group_B)',\n",
       "  'SAR324_clade(Marine_group_B)',\n",
       "  'SAR324_clade(Marine_group_B)',\n",
       "  'SAR324_clade(Marine_group_B)',\n",
       "  'hydrothermal_vent'],\n",
       " ['Bacteria',\n",
       "  'Poribacteria',\n",
       "  'Poribacteria',\n",
       "  'Poribacteria',\n",
       "  'Poribacteria',\n",
       "  'Poribacteria',\n",
       "  'hydrothermal_vent'],\n",
       " ['Bacteria',\n",
       "  'MBNT15',\n",
       "  'MBNT15',\n",
       "  'MBNT15',\n",
       "  'MBNT15',\n",
       "  'MBNT15',\n",
       "  'delta_proteobacterium'],\n",
       " ['Bacteria',\n",
       "  'SAR324_clade(Marine_group_B)',\n",
       "  'SAR324_clade(Marine_group_B)',\n",
       "  'SAR324_clade(Marine_group_B)',\n",
       "  'SAR324_clade(Marine_group_B)',\n",
       "  'SAR324_clade(Marine_group_B)',\n",
       "  'bacterium_associated'],\n",
       " ['Archaea',\n",
       "  'Hadarchaeota',\n",
       "  'Hadarchaeia',\n",
       "  'Hadarchaeia',\n",
       "  'Hadarchaeales',\n",
       "  'Hadarchaeales',\n",
       "  'candidate_divison'],\n",
       " ['Archaea',\n",
       "  'Hadarchaeota',\n",
       "  'Hadarchaeia',\n",
       "  'Hadarchaeia',\n",
       "  'Hadarchaeales',\n",
       "  'Hadarchaeales',\n",
       "  'candidate_divison'],\n",
       " ['Archaea',\n",
       "  'Hadarchaeota',\n",
       "  'Hadarchaeia',\n",
       "  'Hadarchaeia',\n",
       "  'Hadarchaeales',\n",
       "  'Hadarchaeales',\n",
       "  'candidate_divison'],\n",
       " ['Archaea',\n",
       "  'Hadarchaeota',\n",
       "  'Hadarchaeia',\n",
       "  'Hadarchaeia',\n",
       "  'Hadarchaeales',\n",
       "  'Hadarchaeales',\n",
       "  'candidate_divison'],\n",
       " ['Archaea',\n",
       "  'Hadarchaeota',\n",
       "  'Hadarchaeia',\n",
       "  'Hadarchaeia',\n",
       "  'Hadarchaeales',\n",
       "  'Hadarchaeales',\n",
       "  'candidate_divison'],\n",
       " ['Archaea',\n",
       "  'Hadarchaeota',\n",
       "  'Hadarchaeia',\n",
       "  'Hadarchaeia',\n",
       "  'Hadarchaeales',\n",
       "  'Hadarchaeales',\n",
       "  'candidate_divison'],\n",
       " ['Archaea',\n",
       "  'Hadarchaeota',\n",
       "  'Hadarchaeia',\n",
       "  'Hadarchaeia',\n",
       "  'Hadarchaeales',\n",
       "  'Hadarchaeales',\n",
       "  'candidate_divison'],\n",
       " ['Archaea',\n",
       "  'Hadarchaeota',\n",
       "  'Hadarchaeia',\n",
       "  'Hadarchaeia',\n",
       "  'Hadarchaeales',\n",
       "  'Hadarchaeales',\n",
       "  'candidate_divison'],\n",
       " ['Archaea',\n",
       "  'Hadarchaeota',\n",
       "  'Hadarchaeia',\n",
       "  'Hadarchaeia',\n",
       "  'Hadarchaeales',\n",
       "  'Hadarchaeales',\n",
       "  'candidate_divison'],\n",
       " ['Bacteria',\n",
       "  'WOR-1',\n",
       "  'WOR-1',\n",
       "  'WOR-1',\n",
       "  'WOR-1',\n",
       "  'WOR-1',\n",
       "  'candidate_division'],\n",
       " ['Bacteria', 'TA06', 'TA06', 'TA06', 'TA06', 'TA06', 'candidate_division'],\n",
       " ['Bacteria', 'TA06', 'TA06', 'TA06', 'TA06', 'TA06', 'candidate_division'],\n",
       " ['Bacteria',\n",
       "  'RCP2-54',\n",
       "  'RCP2-54',\n",
       "  'RCP2-54',\n",
       "  'RCP2-54',\n",
       "  'RCP2-54',\n",
       "  'hydrothermal_vent'],\n",
       " ['Bacteria',\n",
       "  'Marinimicrobia_(SAR406_clade)',\n",
       "  'Marinimicrobia_(SAR406_clade)',\n",
       "  'Marinimicrobia_(SAR406_clade)',\n",
       "  'Marinimicrobia_(SAR406_clade)',\n",
       "  'Marinimicrobia_(SAR406_clade)',\n",
       "  'hydrothermal_vent'],\n",
       " ['Bacteria',\n",
       "  'AncK6',\n",
       "  'AncK6',\n",
       "  'AncK6',\n",
       "  'AncK6',\n",
       "  'AncK6',\n",
       "  'hydrothermal_vent'],\n",
       " ['Bacteria',\n",
       "  'Marinimicrobia_(SAR406_clade)',\n",
       "  'Marinimicrobia_(SAR406_clade)',\n",
       "  'Marinimicrobia_(SAR406_clade)',\n",
       "  'Marinimicrobia_(SAR406_clade)',\n",
       "  'Marinimicrobia_(SAR406_clade)',\n",
       "  'hydrothermal_vent'],\n",
       " ['Bacteria',\n",
       "  'NB1-j',\n",
       "  'NB1-j',\n",
       "  'NB1-j',\n",
       "  'NB1-j',\n",
       "  'NB1-j',\n",
       "  'hydrothermal_vent'],\n",
       " ['Bacteria',\n",
       "  'NB1-j',\n",
       "  'NB1-j',\n",
       "  'NB1-j',\n",
       "  'NB1-j',\n",
       "  'NB1-j',\n",
       "  'hydrothermal_vent'],\n",
       " ['Bacteria',\n",
       "  'PAUC34f',\n",
       "  'PAUC34f',\n",
       "  'PAUC34f',\n",
       "  'PAUC34f',\n",
       "  'PAUC34f',\n",
       "  'hydrothermal_vent'],\n",
       " ['Bacteria',\n",
       "  'Marinimicrobia_(SAR406_clade)',\n",
       "  'Marinimicrobia_(SAR406_clade)',\n",
       "  'Marinimicrobia_(SAR406_clade)',\n",
       "  'Marinimicrobia_(SAR406_clade)',\n",
       "  'Marinimicrobia_(SAR406_clade)',\n",
       "  'hydrothermal_vent'],\n",
       " ['Bacteria',\n",
       "  'WOR-1',\n",
       "  'WOR-1',\n",
       "  'WOR-1',\n",
       "  'WOR-1',\n",
       "  'WOR-1',\n",
       "  'candidate_division'],\n",
       " ['Bacteria',\n",
       "  'WOR-1',\n",
       "  'WOR-1',\n",
       "  'WOR-1',\n",
       "  'WOR-1',\n",
       "  'WOR-1',\n",
       "  'candidate_division'],\n",
       " ['Bacteria',\n",
       "  'WOR-1',\n",
       "  'WOR-1',\n",
       "  'WOR-1',\n",
       "  'WOR-1',\n",
       "  'WOR-1',\n",
       "  'candidate_division'],\n",
       " ['Bacteria',\n",
       "  'WOR-1',\n",
       "  'WOR-1',\n",
       "  'WOR-1',\n",
       "  'WOR-1',\n",
       "  'WOR-1',\n",
       "  'candidate_division'],\n",
       " ['Bacteria',\n",
       "  'WOR-1',\n",
       "  'WOR-1',\n",
       "  'WOR-1',\n",
       "  'WOR-1',\n",
       "  'WOR-1',\n",
       "  'candidate_division'],\n",
       " ['Bacteria',\n",
       "  'WOR-1',\n",
       "  'WOR-1',\n",
       "  'WOR-1',\n",
       "  'WOR-1',\n",
       "  'WOR-1',\n",
       "  'candidate_division'],\n",
       " ['Bacteria',\n",
       "  'Caldatribacteriota',\n",
       "  'JS1',\n",
       "  'JS1',\n",
       "  'JS1',\n",
       "  'JS1',\n",
       "  'Candidatus_Atribacteria']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore not found taxa\n",
    "not_found_taxa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92997200",
   "metadata": {},
   "source": [
    "#### Filter bacterium and archaeon species with only kingdom information  \n",
    "There are some bacterias and archaeas that only contains information at kingdom and species levels, e.g. `k__Bacteria,..., s__bacterium_Te63R`. These entries are not informative. We will delete them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "168c596e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original size of silva: 76932\n",
      "Final size of silva: 75061\n",
      "Total deleted taxa: 1871\n",
      "2.432017885925233 % of the taxa was deleted\n",
      "\n",
      "Original size of gg: 20441\n",
      "Final size of gg: 20441\n",
      "Total deleted taxa: 0\n",
      "0.0 % of the taxa was deleted\n",
      "\n",
      "Original size of rdp: 20151\n",
      "Final size of rdp: 20151\n",
      "Total deleted taxa: 0\n",
      "0.0 % of the taxa was deleted\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key, db in dbs.items():\n",
    "    # Information\n",
    "    deleted_items = 0\n",
    "    len_original_db = len(db)\n",
    "    print(f\"Original size of {key}: {len_original_db}\")\n",
    "    \n",
    "    # Copy database\n",
    "    db_copy = db.copy()\n",
    "    \n",
    "    for ID, items in db.items():\n",
    "        # if pattern is found and only kingdom information is available\n",
    "        pattern_found = search('^bacterium|^archaeon', items['taxa'][6])\n",
    "        notonly_kingdom_info = sum(bool(tax) for tax in items['taxa'][1:6])\n",
    "        \n",
    "        if pattern_found and not notonly_kingdom_info:\n",
    "            # delete entry\n",
    "            db_copy.pop(ID)\n",
    "            \n",
    "            # Information\n",
    "            deleted_items = deleted_items + 1\n",
    "    \n",
    "    dbs[key] = db_copy\n",
    "    \n",
    "    # Information\n",
    "    len_db = len(db_copy)\n",
    "    print(f\"Final size of {key}: {len_db}\")\n",
    "    print(f\"Total deleted taxa: {deleted_items}\")\n",
    "    print(f\"{(deleted_items/len_original_db)*100} % of the taxa was deleted\\n\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfabacb0",
   "metadata": {},
   "source": [
    "#### Filter Kingdom  \n",
    "Some eukaryota species where classified as bacteria (because the sequence belonged to a chloroplast o a mitochondria or a parasite of the eukaryota i guess) and with the ncbi taxonomy updated they have been reclassified as eukaryota. \n",
    "As we are not interested in eukaryota we will delete this entries. \n",
    "\n",
    "UPDATE: With further observations of the results, we have also noticed some Virus and kingdom unknown entries. Therefore we will only keep entries from kingdoms Bacteria and Archaea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "484e4f4f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original size of silva: 75061\n",
      "Final size of silva: 74616\n",
      "Total deleted taxa: 445\n",
      "0.592851147733177 % of the taxa was deleted\n",
      "\n",
      "Original size of gg: 20441\n",
      "Final size of gg: 20434\n",
      "Total deleted taxa: 7\n",
      "0.03424489995597084 % of the taxa was deleted\n",
      "\n",
      "Original size of rdp: 20151\n",
      "Final size of rdp: 20151\n",
      "Total deleted taxa: 0\n",
      "0.0 % of the taxa was deleted\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key, db in dbs.items():\n",
    "    # Information\n",
    "    deleted_items = 0\n",
    "    len_original_db = len(db)\n",
    "    print(f\"Original size of {key}: {len_original_db}\")\n",
    "    \n",
    "    # Copy database\n",
    "    db_copy = db.copy()\n",
    "    \n",
    "    for ID, items in db.items():\n",
    "        # if pattern is found \n",
    "        \n",
    "        if items['taxa'][0] not in [\"Bacteria\", \"Archaea\"]:\n",
    "            # delete entry\n",
    "            db_copy.pop(ID)            \n",
    "            # Information\n",
    "            deleted_items = deleted_items + 1\n",
    "    \n",
    "    dbs[key] = db_copy\n",
    "    \n",
    "    # Information\n",
    "    len_db = len(db_copy)\n",
    "    print(f\"Final size of {key}: {len_db}\")\n",
    "    print(f\"Total deleted taxa: {deleted_items}\")\n",
    "    print(f\"{(deleted_items/len_original_db)*100} % of the taxa was deleted\\n\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a45ebe",
   "metadata": {},
   "source": [
    "#### Save Updated taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ca1244b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbs_obj = join(or_dir, \"dbs_dict_updated.pickle\")\n",
    "pickle.dump(dbs, open(dbs_obj, 'wb' ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce8e9fa",
   "metadata": {},
   "source": [
    "### Merge databases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0b1f8d",
   "metadata": {},
   "source": [
    "#### Change environment \n",
    " For further analysis, change to qiime2 environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "709fcfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from os.path import join, basename, splitext, exists\n",
    "from copy import deepcopy\n",
    "\n",
    "from Bio import SeqIO\n",
    "from Bio import Entrez\n",
    "\n",
    "import new_approach\n",
    "from qiime2 import Artifact\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import utils\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a191aeff",
   "metadata": {},
   "source": [
    "#### Load databases object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c823d5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "or_dir = \"original_db\"\n",
    "dbs_obj = join(or_dir, \"dbs_dict_updated.pickle\")\n",
    "dbs = pickle.load(open(dbs_obj, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51a60c3",
   "metadata": {},
   "source": [
    "#### Merge RDP and Silva databases\n",
    "We will use RDP as base database and we will add Silva entries with the following approach\n",
    " - For each entry in Silva look if the taxonomy is present in RDP.\n",
    "     - NO: add entry to the database\n",
    "     - YES: if there is already a taxonomy with this sequence\n",
    "         - YES: skip\n",
    "         - NO: add a new entry to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26ac5ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_db = dbs['rdp']\n",
    "candidate_db = dbs['silva']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d33881a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merged db with initial SILVA entries\n",
    "merged_db = deepcopy(base_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0046a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial len: 20151\n",
      "Final len: 72721\n",
      "Number of new taxonomies added: 3092\n",
      "Number of new sequences added: 52570\n",
      "Number of new substr found: 6523\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "merged_db = new_approach.integrate_db(merged_db, candidate_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9623c6",
   "metadata": {},
   "source": [
    "#### Add Greengenes database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a132808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial len: 72721\n",
      "Final len: 90269\n",
      "Number of new taxonomies added: 288\n",
      "Number of new sequences added: 17548\n",
      "Number of new substr found: 1396\n"
     ]
    }
   ],
   "source": [
    "candidate_db = dbs['gg']\n",
    "merged_db = new_approach.integrate_db(merged_db, candidate_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0301a0be",
   "metadata": {},
   "source": [
    "#### Download full 16S merged database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc725699",
   "metadata": {},
   "outputs": [],
   "source": [
    "created_db = \"created_db\"\n",
    "taxa_out = join(created_db, \"gsr_full-16S_taxa.txt\")\n",
    "seqs_out = join(created_db, \"gsr_full-16S_seqs.fasta\")\n",
    "utils.download_db_from_dict(merged_db, taxa_out, seqs_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af54b0d",
   "metadata": {},
   "source": [
    "## Add relevant vaginal species\n",
    "### Obtain relevant species for vaginal\n",
    "We obtain the relevant names form the supplementary taxa of this [article](https://doi.org/10.1186/1471-2164-13-S8-S17)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "283022df",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vaginal = \"vaginal/vaginal_db.csv\"\n",
    "vaginal_df = pd.read_csv(input_vaginal, header = 2)\n",
    "vaginal_df = vaginal_df[['GenBank Accession Number', 'Species-Level Taxon*']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e146b2da",
   "metadata": {},
   "source": [
    "#### Preprocess species names: delete cluster and OTU terminologys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fe116c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vaginal_df.replace(\" cluster.*| OTU.*\", \"\", inplace=True, regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1bf200",
   "metadata": {},
   "source": [
    "#### Delete family names form nomenclature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0dbf95bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vaginal_df.replace(\".*aceae.*\", \"\", regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227dc4a1",
   "metadata": {},
   "source": [
    "#### Delete empty rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9c31867",
   "metadata": {},
   "outputs": [],
   "source": [
    "vaginal_df.replace(\"\", np.nan, inplace=True)\n",
    "vaginal_df.replace(\"-\", np.nan, inplace=True)\n",
    "vaginal_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cda5f5",
   "metadata": {},
   "source": [
    "#### Keep only rows containning species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c5d3693",
   "metadata": {},
   "outputs": [],
   "source": [
    "vaginal_df['length'] = vaginal_df['Species-Level Taxon*'].apply(lambda x: len(x.split(\" \")))\n",
    "vaginal_df = vaginal_df[vaginal_df['length'] > 1]\n",
    "vaginal_df = vaginal_df[['GenBank Accession Number', 'Species-Level Taxon*']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4aab89",
   "metadata": {},
   "source": [
    "#### Save list of vaginal species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb831a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vagina_out = \"vaginal/vaginal_filtered_species.csv\"\n",
    "if not exists(vagina_out):\n",
    "    vaginal_df.to_csv(vagina_out, index=False, header=True)\n",
    "else:\n",
    "    vaginal_df = pd.read_csv(vagina_out, header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba1ea1b",
   "metadata": {},
   "source": [
    "### Obtain taxa from NCBI \n",
    "First, we will check in the NCBI if the vaginal_id has some NCBI taxid associated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31d77cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vaginal_sp = list(vaginal_df['Species-Level Taxon*'])\n",
    "vaginal_ids = list(vaginal_df['GenBank Accession Number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "569ddedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Entrez.email = \"leidyalejandra.gonzalez01@estudiant.upf.edu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad83b05",
   "metadata": {},
   "source": [
    "SEVERAL NOTES ON THE CHUNK BELOW\n",
    "1. ncbi api is a bitch\n",
    "2. most of the time it will work but sometimes it will start an infinite loop. SO: check the output and if you see the loop is happening stop and run again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219067c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tax_ids = []\n",
    "\n",
    "# FIND NCBI TAX IDS\n",
    "\n",
    "for i in range(len(vaginal_ids)):\n",
    "    \n",
    "    # 1. search the taxonomy id for the genbank id\n",
    "    # we are sure that genbank ids exist in ncbi database, so the result cannot be empty \n",
    "    # if the result is empty (aka record[0] raises a Index error) try again\n",
    "    # it no error is raised, exit the loop\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            record = Entrez.read(Entrez.elink(db=\"taxonomy\", dbfrom=\"nucleotide\", id=vaginal_ids[i]))\n",
    "            record[0]\n",
    "        except IndexError:\n",
    "            print(f\"{vaginal_ids[i]} entry not found, trying again\")\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"{vaginal_ids[i]} entry found\")\n",
    "            break \n",
    "    \n",
    "    # 2. Look for the taxaid in the result\n",
    "    # if the result contains the taxID, save it\n",
    "    # if not, 2 things may have happened:\n",
    "        # 1: the sequence has been updated and we need to search the updated genbank ID\n",
    "        # 2: an error occurred *angry noises* (YES IT HAPPENS SOMETIMES)\n",
    "    # To be sure that it is not an error, we will repeat the search a maximum of 3 times\n",
    "    # (there is still a possibility that the error occurs but it is small)\n",
    "    \n",
    "    \n",
    "    # If record found\n",
    "    if record[0]['LinkSetDb']:\n",
    "        tax_ids.append(record[0]['LinkSetDb'][0]['Link'][0]['Id'])\n",
    "        \n",
    "    # Not found: \n",
    "    else:\n",
    "        \n",
    "        # try 3 more times to check it is not an error:\n",
    "        for x in range(3):\n",
    "            print(f\"{vaginal_ids[i]} taxonomy not found, trying again\")\n",
    "            record = Entrez.read(Entrez.elink(db=\"taxonomy\", dbfrom=\"nucleotide\", id=vaginal_ids[i]))\n",
    "            # if found end loop\n",
    "            if record[0]['LinkSetDb']:\n",
    "                print(f\"{vaginal_ids[i]} taxonomy found\")\n",
    "                break\n",
    "        \n",
    "        \n",
    "        # If now taxonomy is found: save\n",
    "        if record[0]['LinkSetDb']:\n",
    "            tax_ids.append(record[0]['LinkSetDb'][0]['Link'][0]['Id'])\n",
    "            \n",
    "        # If taxonomy still not found: updated sequence\n",
    "        else:\n",
    "            print(f\"{vaginal_ids[i]} taxonomy not found, trying updated version\")\n",
    "            # Change extension to updated version (1 to 2)\n",
    "            vaginal_ids[i] = f\"{vaginal_ids[i][:-1]}2\"\n",
    "            \n",
    "            # search with the updated id until it is found\n",
    "            \n",
    "            while True:\n",
    "                try:\n",
    "                    record = Entrez.read(Entrez.elink(db=\"taxonomy\", dbfrom=\"nucleotide\", id=vaginal_ids[i]))\n",
    "                    record[0]['LinkSetDb']\n",
    "                except IndexError:\n",
    "                    print(f\"{vaginal_ids[i]} entry not found, trying again\")\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f\"{vaginal_ids[i]} entry found\")\n",
    "                    break\n",
    "                \n",
    "            # if it has taxonomy,save, if not fuck it\n",
    "\n",
    "            if record[0]['LinkSetDb']:\n",
    "                tax_ids.append(record[0]['LinkSetDb'][0]['Link'][0]['Id'])\n",
    "            else:\n",
    "                print(f\"{vaginal_ids[i]} taxonomy not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc055f5c",
   "metadata": {},
   "source": [
    "Looking manually in the NCBI Nucleotide database, we suspected that the majority of not found records correspond to updated sequences. As an example, `M59083.1` sequence was updated to `M59083.2`, changing its extension id from `.1` to `.2`. Therefore we will change the extension of the not found records in order to search them again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39e9f3c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tax_ids) == len(vaginal_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c6778f",
   "metadata": {},
   "source": [
    "Downloand objects. Note: for some unknown and strange reason, we weren't able to pickle de tax_ids, so, we decided to save the variable in a script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90d204c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vaginal_id_obj = join(\"vaginal\", \"vaginal_ids.pickle\")\n",
    "pickle.dump(vaginal_ids, open(vaginal_id_obj, 'wb' ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b3dcd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wtf\n",
    "with open(\"vaginal/vaginal_sp_taxid.py\", \"w\") as file:\n",
    "    file.write(f\"tax_ids = {tax_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67c64a7",
   "metadata": {},
   "source": [
    "### Create vaginal database\n",
    "**Change to ete3 environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae953872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from os.path import join, basename\n",
    "from re import search\n",
    "\n",
    "from Bio import Entrez\n",
    "from Bio import SeqIO\n",
    "\n",
    "import update_taxonomy\n",
    "import utils\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7796c7",
   "metadata": {},
   "source": [
    "#### Load sequence ids and taxonomy ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31a06e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vaginal_id_obj = join(\"vaginal\", \"vaginal_ids.pickle\")\n",
    "vaginal_ids = pickle.load(open(vaginal_id_obj, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70be2039",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaginal.vaginal_sp_taxid import tax_ids\n",
    "vaginal_txid = tax_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499ca104",
   "metadata": {},
   "source": [
    "#### Obtain sequences from the ncbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4b06780",
   "metadata": {},
   "outputs": [],
   "source": [
    "Entrez.email = \"leidyalejandra.gonzalez01@estudiant.upf.edu\"\n",
    "# Get Seqs from NCBI\n",
    "fasta_fh = Entrez.efetch(db=\"nucleotide\", id=vaginal_ids, rettype=\"fasta\", retmode=\"text\")\n",
    "\n",
    "ncbi_seqs = SeqIO.to_dict(SeqIO.parse(fasta_fh, \"fasta\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb6b5d5",
   "metadata": {},
   "source": [
    "#### Create vaginal database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "122c147b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vaginal_seqs = \"vaginal/vaginal_seqs.fasta\"\n",
    "vaginal_tax = \"vaginal/vaginal_taxa.txt\"\n",
    "vaginal_db = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3cc607d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial sequences: 634\n",
      "Discarded sequences (complete genomes or unknown species): 15\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "del_seqs = 0\n",
    "print(f\"Initial sequences: {len(vaginal_ids)}\")\n",
    "\n",
    "for ID, tax_id in zip(vaginal_ids, vaginal_txid):\n",
    "    seq = ncbi_seqs[ID].seq\n",
    "    taxa = update_taxonomy.get_taxa_from_specie_taxid(tax_id)\n",
    "    \n",
    "    # discard complete genomes and unknown species\n",
    "    if len(seq) < 2000 and taxa[6] != 'uncultured_bacterium' : \n",
    "        # add entry\n",
    "        vaginal_db[ID] = {'seq': seq, 'taxa': taxa}     \n",
    "       \n",
    "    else:\n",
    "        del_seqs = del_seqs + 1\n",
    "\n",
    "print(f\"Discarded sequences (complete genomes or unknown species): {del_seqs}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b8e81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vaginal_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f664f96",
   "metadata": {},
   "source": [
    "#### Downloand vaginal database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7f0dd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.download_db_from_dict(vaginal_db, vaginal_tax, vaginal_seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f9d940",
   "metadata": {},
   "source": [
    "### Integrate vaginal db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98a0e58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to qiime2 env \n",
    "import pickle\n",
    "from os.path import join, basename, splitext, exists\n",
    "\n",
    "from Bio import SeqIO\n",
    "from Bio import Entrez\n",
    "\n",
    "from libs import new_approach\n",
    "from qiime2 import Artifact\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b34a58b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial len: 90269\n",
      "Final len: 90408\n",
      "Number of new taxonomies added: 8\n",
      "Number of new sequences added: 139\n",
      "Number of new substr found: 57\n"
     ]
    }
   ],
   "source": [
    "# prevously merged database (full 16S database)\n",
    "taxa_16S = \"created_db/gsr_full-16S_taxa.txt\"\n",
    "seqs_16S = \"created_db/gsr_full-16S_seqs.fasta\"\n",
    "# vaginal database\n",
    "vaginal_seqs = \"vaginal/vaginal_seqs.fasta\"\n",
    "vaginal_tax = \"vaginal/vaginal_taxa.txt\"\n",
    "\n",
    "merged_db = utils.load_db_from_files(taxa_16S, seqs_16S)\n",
    "vaginal_db = utils.load_db_from_files(vaginal_tax, vaginal_seqs)\n",
    "\n",
    "# set(merged_db.keys()).intersection(set(vaginal_db.keys()))\n",
    "\n",
    "merged_db = new_approach.integrate_db(merged_db, vaginal_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de21de88",
   "metadata": {},
   "source": [
    "#### Download vaginal integrated db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb3f9673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'created_db/gsrv_full-16S_seqs.qza'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxa_out = \"created_db/gsrv_full-16S_taxa.txt\"\n",
    "seqs_out = \"created_db/gsrv_full-16S_seqs.fasta\"\n",
    "\n",
    "utils.download_db_from_dict(merged_db, taxa_out, seqs_out)\n",
    "\n",
    "# convert files to qza\n",
    "utils.file_to_qza(taxa_out)\n",
    "utils.file_to_qza(seqs_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2c4c5c",
   "metadata": {},
   "source": [
    "# Filter full-16S database by length \n",
    "\n",
    "As we have seen in the [database stats](gsrv_stats.ipynb) notebook, full-16S database has some entries with lengths that are not probable to correspond to 16S rRNA sequence. For this reason, we will filter out too short and too long sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d1add6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import check_dir\n",
    "import utils\n",
    "from os.path import join, exists, basename, splitext\n",
    "import pickle\n",
    "from re import sub, search\n",
    "from copy import copy\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ed0a924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load database\n",
    "taxa_path = \"created_db/gsrv_full-16S_taxa.txt\"\n",
    "seqs_path = \"created_db/gsrv_full-16S_seqs.fasta\"\n",
    "db = utils.load_db_from_files(taxa_path=taxa_path, seqs_path=seqs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53e32666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter database by sequence length\n",
    "minlen = 900\n",
    "maxlen = 1800\n",
    "\n",
    "db_filt = db.copy()\n",
    "\n",
    "for ID, items in db.items():\n",
    "    seqlen = len(items['seq'])\n",
    "    if seqlen < minlen or seqlen > maxlen:\n",
    "        db_filt.pop(ID, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbdd2cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous gsr size: 90408\n",
      "Filtered gsr size: 90231\n"
     ]
    }
   ],
   "source": [
    "print(f\"Previous gsr size: {len(db)}\")\n",
    "print(f\"Filtered gsr size: {len(db_filt)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1032bfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxa_out = \"created_db/gsrv_full-16S_filt_taxa.txt\"\n",
    "seqs_out = \"created_db/gsrv_full-16S_filt_seqs.fasta\"\n",
    "\n",
    "utils.download_db_from_dict(db_filt, taxa_out, seqs_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d09eed3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'created_db/gsrv_full-16S_filt_seqs.qza'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert files to qza\n",
    "utils.file_to_qza(taxa_out)\n",
    "utils.file_to_qza(seqs_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c213480f",
   "metadata": {},
   "source": [
    "# Accomodate Metasquare taxonomy for qiime training and classification\n",
    "\n",
    "Many Metasquare entries have incomplete taxonomical anotations (aka they are missing taxonomical levels) and this becomes problematic when performing classification with qiime2. Therefore here we complete all metasquare taxonomies up to species level, filling the missing levels with \"unknown\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b312f0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1d386f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load taxa file\n",
    "ms_taxafile = 'created_db/metasquare_V4_taxa.txt'\n",
    "\n",
    "ms_taxa = utils.load_db_from_files(taxa_path=ms_taxafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "42638a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill taxonomies\n",
    "for key in ms_taxa.keys():\n",
    "    a = ms_taxa[key]['taxa'] \n",
    "    ms_taxa[key]['taxa'] = [a[i] if i < len(a) else 'unknown' for i in range(0,7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5b442afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download formated taxa\n",
    "ms_taxa_out = 'created_db/metasquare_V4_taxa_format.txt'\n",
    "utils.download_db_from_dict(d = ms_taxa, taxa_out_file=ms_taxa_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8cd8bb33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'created_db/metasquare_V4_taxa_format.qza'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import to qza\n",
    "utils.file_to_qza(ms_taxa_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7702163e",
   "metadata": {},
   "source": [
    "# Create taxonomy file for GTDB + format sequence file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7cff6943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from Bio import SeqIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "04145537",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file = 'original_db/gtdb_r207_seqs.fasta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "23ffc6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create database\n",
    "gtdb = {}\n",
    "\n",
    "with open(in_file, \"r\") as seqsfile:\n",
    "    for record in SeqIO.parse(seqsfile, \"fasta\"):\n",
    "        ID = record.id\n",
    "        seq= record.seq\n",
    "        desc = record.description\n",
    "        taxa = \"_\".join(desc.split(' ')[1:-3])\n",
    "        taxa = taxa.split(';')\n",
    "        taxa = utils.rm_prefs(taxa)\n",
    "        \n",
    "        gtdb[ID] = {}\n",
    "        gtdb[ID]['taxa'] = taxa\n",
    "        gtdb[ID]['seq'] = seq\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a3922340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download database\n",
    "gtdb_taxa_out = 'original_db/gtdb_full-16S_taxa.txt'\n",
    "gtdb_seq_out = 'original_db/gtdb_full-16S_seqs.fasta'\n",
    "\n",
    "utils.download_db_from_dict(d=gtdb, taxa_out_file=gtdb_taxa_out, seqs_out_file=gtdb_seq_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "14443430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'original_db/gtdb_full-16S_seqs.qza'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import to qza\n",
    "utils.file_to_qza(gtdb_taxa_out)\n",
    "utils.file_to_qza(gtdb_seq_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "510px",
    "width": "543px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
